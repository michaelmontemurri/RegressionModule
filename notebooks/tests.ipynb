{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Module Documentation\n",
    "MATH 533 Assignment 4\n",
    "\n",
    "December 20th, 2024\n",
    "\n",
    "Gulliver Hager, Evelyn Hubbard, Michael Montemurri\n",
    "\n",
    "---\n",
    "## models.py\n",
    "\n",
    "This file provides examples and documentation for the key methods implemented in the `OLS`, `GLS`, and `Ridge` regression models. The examples illustrate how to use these methods, including fitting models, making predictions, and obtaining variance estimates.\n",
    "\n",
    "### Class: OLS(include_intercept)\n",
    "Ordinary Least Squares (OLS) regression model.\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term, default = true.\n",
    "#### Attributes:\n",
    "- **beta** (`numpy.ndarray`): Coefficients of the fitted model.\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term.\n",
    "#### Methods:\n",
    "##### `OLS.fit(X, y, use_gradient_descent=False, max_iter=1000, alpha=0.01, tol=1e-6)`\n",
    "Fits the Ordinary Least Squares (OLS) model to the dataset.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "- **use_gradient_descent** (`bool`, optional): Whether to use gradient descent for optimization (default: False).\n",
    "- **max_iter** (`int`, optional): Maximum iterations for gradient descent (default: 1000).\n",
    "- **alpha** (`float`, optional): Learning rate for gradient descent (default: 0.01).\n",
    "- **tol** (`float`, optional): Convergence tolerance for gradient descent (default: 1e-6).\n",
    "###### Returns:\n",
    "None. Fits the model and stores coefficients in `self.beta`.\n",
    "\n",
    "##### `OLS.predict(X)`\n",
    "Predicts response values using the fitted OLS model.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "###### Returns:\n",
    "- **predictions** (`numpy.ndarray`): Predicted values for each sample.\n",
    "\n",
    "##### `OLS.estimate_variance(X, y)`\n",
    "Estimates the variance-covariance matrix of the coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "###### Returns:\n",
    "- **variance_matrix** (`numpy.ndarray`): Variance-covariance matrix of the coefficients.\n",
    "\n",
    "### Class: GLS(include_intercept)\n",
    "Generalized Least Squares (GLS) regression model.\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term, default = true.\n",
    "#### Attributes:\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term.\n",
    "- **beta** (`numpy.ndarray`): Coefficients of the fitted model.\n",
    "- **sigma** (`numpy.ndarray`): Covariance matrix of the errors.\n",
    "#### Methods:\n",
    "##### `GLS.fit(X, y, sigma)`\n",
    "Fits the Generalized Least Squares (GLS) model to the dataset.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "- **sigma** (`numpy.ndarray`): Covariance matrix of the errors.\n",
    "###### Returns:\n",
    "None. Fits the model and stores coefficients in `self.beta`.\n",
    "\n",
    "### Class: Ridge(lambda, include_intercept)\n",
    "Ridge regression model.\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term, default = true.\n",
    "- **lambda** (`float`): $\\lambda$ for the Ridge Estimator. \n",
    "#### Attributes:\n",
    "- **beta** (`numpy.ndarray`): Coefficients of the fitted model.\n",
    "- **alpha** (`float`): Regularization strength.\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term.\n",
    "#### Methods:\n",
    "##### `Ridge.fit(X, y)`\n",
    "Fits the Ridge regression model to the dataset.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "###### Returns:\n",
    "None. Fits the model and stores coefficients in `self.beta`.\n",
    "##### `Ridge.estimate_variance(X, y)`\n",
    "Estimates the variance-covariance matrix of the coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "###### Returns:\n",
    "- **variance_matrix** (`numpy.ndarray`): Variance-covariance matrix of the coefficients.\n",
    "\n",
    "### Class: ReducedModel(base_model, selected_features)\n",
    "Wrapper class for reduced models.\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **base_model** (`object`): The base model instance (e.g., OLS, Ridge, etc.).\n",
    "- **selected_features** (`list`): List of selected feature indices for the reduced model (e.g., [0,1,2]).\n",
    "#### Attributes:\n",
    "- **base_model** (`object`): The base model instance.\n",
    "- **selected_features** (`list`): List of selected feature indices.\n",
    "#### Methods:\n",
    "##### `ReducedModel.fit(X, y)`\n",
    "Fits the reduced model to the data.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "###### Returns:\n",
    "None. Fits the model and stores coefficients in `self.base_model.beta`.\n",
    "\n",
    "##### `ReducedModel.predict(X)`\n",
    "Predicts response values using the fitted reduced model.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "###### Returns:\n",
    "- **predictions** (`numpy.ndarray`): Predicted values for each sample.\n",
    "\n",
    "##### `ReducedModel.beta`\n",
    "Returns the coefficients of the fitted reduced model.\n",
    "###### Returns:\n",
    "- **beta** (`numpy.ndarray`): Coefficients of the fitted reduced model.\n",
    "\n",
    "### Function: summary(model,X,y)\n",
    "Generate a summary of the model's performance.\n",
    "#### Parameters:\n",
    "- **model** (`object`) : The regression model (OLS, GLS, or Ridge). Must have a `predict` method.\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "\n",
    "#### Returns\n",
    "- A dictionary containing:\n",
    "    - **coefficients** (`numpy.ndarray`): The fitted coefficients of the model.\n",
    "    - **r_squared** (`float`): The R-squared value, representing the proportion of variance explained by the model.\n",
    "\n",
    "---\n",
    "## Example\n",
    "\n",
    "Below is an example demonstrating the use of `OLS`, `GLS`, and `Ridge` regression models using synthetic datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from stats_module import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Coefficients: [-0.14050914  0.31998031 -0.04676428  1.07625074 -0.2476699  -0.28304471]\n",
      "OLS Variance Matrix:\n",
      " [[ 0.26947461  0.01435536  0.02568034 -0.02309508  0.03055034]\n",
      " [ 0.01435536  0.30429335  0.03317391 -0.01444322 -0.03871695]\n",
      " [ 0.02568034  0.03317391  0.27349674 -0.00615378 -0.0117382 ]\n",
      " [-0.02309508 -0.01444322 -0.00615378  0.25447581 -0.02888467]\n",
      " [ 0.03055034 -0.03871695 -0.0117382  -0.02888467  0.29307544]]\n",
      "\n",
      "OLS Summary:\n",
      "beta_hat: [-0.14050914  0.31998031 -0.04676428  1.07625074 -0.2476699  -0.28304471]\n",
      "R-squared: 0.8405222067551491\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 5\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_beta = np.random.randn(n_features)\n",
    "y = X @ true_beta + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# OLS Example\n",
    "ols_model = OLS(include_intercept=True)\n",
    "ols_model.fit(X, y)\n",
    "ols_predictions = ols_model.predict(X)\n",
    "ols_variance = ols_model.estimate_variance(X, y)\n",
    "print(\"OLS Coefficients:\", ols_model.beta)\n",
    "print(\"OLS Variance Matrix:\\n\", ols_variance)\n",
    "\n",
    "#get summary\n",
    "ols_summary = summary(ols_model, X, y)\n",
    "print(\"\\nOLS Summary:\")\n",
    "print(f\"beta_hat: {ols_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ols_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 37 iterations\n",
      "GD Coefficients: [-0.14112852  0.31944985 -0.04772522  1.07540995 -0.24760737 -0.28247484]\n",
      "\n",
      "GD Summary:\n",
      "beta_hat: [-0.14112852  0.31944985 -0.04772522  1.07540995 -0.24760737 -0.28247484]\n",
      "R-squared: 0.840521026564396\n"
     ]
    }
   ],
   "source": [
    "# Now using the gradient descent method\n",
    "gd_model = OLS(include_intercept=True)\n",
    "gd_model.fit(X, y, use_gradient_descent=True, max_iter=1000, alpha=0.1, tol=1e-6)\n",
    "gd_predictions = gd_model.predict(X)\n",
    "gd_variance = gd_model.estimate_variance(X, y)\n",
    "print(\"GD Coefficients:\", gd_model.beta)\n",
    "\n",
    "gd_summary = summary(gd_model, X, y)\n",
    "print(\"\\nGD Summary:\")\n",
    "print(f\"beta_hat: {gd_summary['coefficients']}\")\n",
    "print(f\"R-squared: {gd_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLS Coefficients: [-1.41474337e+02 -1.02416558e+00 -3.39547679e+01 -2.48677846e+01\n",
      " -2.01403798e+02 -8.57498424e+00 -8.65784250e+01  4.79311400e+00\n",
      "  1.54393091e+02  7.27781662e+01 -3.42204323e+01  5.47556296e+01\n",
      "  1.86483905e+01 -1.31787537e+01 -8.12271465e+00 -7.33317058e-03\n",
      " -4.46468035e+01  4.41732350e+01 -9.76029194e+01  9.99925392e+01\n",
      "  1.40768664e+01  4.11415944e+01 -6.22458049e+01 -4.31286647e+01\n",
      " -1.36458050e+01  7.38014585e+00 -5.73388141e+01  4.12521935e+01\n",
      "  1.43603183e+01 -4.36207590e+00 -7.48400271e+00 -2.89754174e+00\n",
      " -7.46537372e+00 -7.65292231e+00 -7.52196805e-01 -4.06161741e+00\n",
      " -2.16262630e+00 -1.57008564e+00 -1.25034785e+01  2.38068227e+01\n",
      " -1.75874823e+01  6.47448554e-01 -1.76569838e+00  1.65438305e+01\n",
      "  1.82114547e+01  4.74034574e+00 -2.08518073e+00  2.35645716e+00\n",
      "  2.89380980e+00  8.58723902e+00  1.41069722e+00]\n",
      "\n",
      "GLS Summary:\n",
      "beta_hat: [-1.41474337e+02 -1.02416558e+00 -3.39547679e+01 -2.48677846e+01\n",
      " -2.01403798e+02 -8.57498424e+00 -8.65784250e+01  4.79311400e+00\n",
      "  1.54393091e+02  7.27781662e+01 -3.42204323e+01  5.47556296e+01\n",
      "  1.86483905e+01 -1.31787537e+01 -8.12271465e+00 -7.33317058e-03\n",
      " -4.46468035e+01  4.41732350e+01 -9.76029194e+01  9.99925392e+01\n",
      "  1.40768664e+01  4.11415944e+01 -6.22458049e+01 -4.31286647e+01\n",
      " -1.36458050e+01  7.38014585e+00 -5.73388141e+01  4.12521935e+01\n",
      "  1.43603183e+01 -4.36207590e+00 -7.48400271e+00 -2.89754174e+00\n",
      " -7.46537372e+00 -7.65292231e+00 -7.52196805e-01 -4.06161741e+00\n",
      " -2.16262630e+00 -1.57008564e+00 -1.25034785e+01  2.38068227e+01\n",
      " -1.75874823e+01  6.47448554e-01 -1.76569838e+00  1.65438305e+01\n",
      "  1.82114547e+01  4.74034574e+00 -2.08518073e+00  2.35645716e+00\n",
      "  2.89380980e+00  8.58723902e+00  1.41069722e+00]\n",
      "R-squared: -72.08208201982067\n"
     ]
    }
   ],
   "source": [
    "# GLS Example\n",
    "sigma = np.diag(np.random.uniform(0.5, 1.5, size=n_samples))\n",
    "gls_model = GLS(include_intercept=True)\n",
    "gls_model.fit(X, y, sigma)\n",
    "gls_predictions = gls_model.predict(X)\n",
    "print(\"GLS Coefficients:\", gls_model.beta)\n",
    "\n",
    "gls_summary = summary(gls_model, X, y)\n",
    "print(\"\\nGLS Summary:\")\n",
    "print(f\"beta_hat: {gls_summary['coefficients']}\")\n",
    "print(f\"R-squared: {gls_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLS Coefficients: [ 3459.43014604   781.3586428   1660.62118543    63.32927029\n",
      "  3245.32414548   293.5061748   1482.64819752   127.06898964\n",
      " -1772.15243155   -77.71718029  2573.7842996  -6866.74429691\n",
      "   918.34342631  2873.19737535  1916.17044252 -2654.44959727\n",
      "  3677.93146342   573.41239077  -621.97401433    82.97666246\n",
      "  -324.30759424  2537.9440874  -1473.6903729    711.01351196\n",
      "  -292.38754577  -312.34023322   132.2966538     89.80961419\n",
      " -1413.38193119   438.74002391  -523.60259323    46.39030624\n",
      "  -245.8091236   -149.62518923    26.18630345    -9.22126557\n",
      "  -157.05515896  -184.11498277   192.89493025   -14.40886809\n",
      "   -84.35492264  -145.55508925    86.59430635  -276.25610193\n",
      "   737.72995118   431.44165972  -225.8688174    953.84772956\n",
      "   956.91190579  -240.8228959    515.5198604 ]\n",
      "OLS Coefficients: [ 3459.43014604   781.3586428   1660.62118543    63.32927029\n",
      "  3245.32414548   293.5061748   1482.64819752   127.06898964\n",
      " -1772.15243155   -77.71718029  2573.7842996  -6866.74429691\n",
      "   918.34342631  2873.19737535  1916.17044252 -2654.44959727\n",
      "  3677.93146342   573.41239077  -621.97401433    82.97666246\n",
      "  -324.30759424  2537.9440874  -1473.6903729    711.01351196\n",
      "  -292.38754577  -312.34023322   132.2966538     89.80961419\n",
      " -1413.38193119   438.74002391  -523.60259323    46.39030624\n",
      "  -245.8091236   -149.62518923    26.18630345    -9.22126557\n",
      "  -157.05515896  -184.11498277   192.89493025   -14.40886809\n",
      "   -84.35492264  -145.55508925    86.59430635  -276.25610193\n",
      "   737.72995118   431.44165972  -225.8688174    953.84772956\n",
      "   956.91190579  -240.8228959    515.5198604 ]\n"
     ]
    }
   ],
   "source": [
    "# We can see that this reduces to OLS when sigma is the identity matrix\n",
    "gls_model = GLS(include_intercept=True)\n",
    "gls_model.fit(X, y, np.eye(n_samples))\n",
    "gls_predictions = gls_model.predict(X)\n",
    "gls_variance = gls_model.estimate_variance(X, y)\n",
    "print(\"GLS Coefficients:\", gls_model.beta)\n",
    "print(\"OLS Coefficients:\", ols_model.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Variance estimator assumes homoskedasticity.\n",
      "Ridge Coefficients: [-7.01463766e-01 -4.14927788e-02  4.85240430e-01  9.87668094e-01\n",
      "  1.25040951e-01 -2.72393501e-01  1.67967585e-01 -3.99480101e-01\n",
      "  6.84333398e-01 -2.69287346e-01 -6.01096888e-01 -4.11982277e-01\n",
      "  2.14168569e-01  4.03590275e-01 -4.27594067e-01  3.51180459e-01\n",
      " -5.16279513e-01 -4.40772801e-01  5.05087869e-01  2.94026386e-01\n",
      "  3.24829802e-03 -1.52542973e-02 -4.43588610e-04 -5.67087078e-01\n",
      "  1.15890212e+00  4.72319497e-01 -1.66810151e-01 -3.94074104e-01\n",
      " -8.92116390e-01  5.99992524e-01  2.08375565e-01  1.48485484e-01\n",
      "  1.10248471e+00 -3.35140429e-01 -3.73342078e-01 -6.93918498e-01\n",
      "  5.26882859e-01  1.59862243e-01  8.46800777e-01  7.58725114e-01\n",
      " -4.00269462e-01 -5.16951742e-01  5.40614982e-01  3.29460454e-02\n",
      "  8.26647902e-01 -1.42721465e+00 -1.66923795e-01 -4.22893444e-01\n",
      " -1.75686068e-01 -8.72660402e-01  3.24860890e-01]\n",
      "Ridge Variance Matrix:\n",
      " [[-0.00576269  0.0008586  -0.00125661 ... -0.00229909  0.00443764\n",
      "  -0.00065676]\n",
      " [ 0.0008586  -0.00450496  0.00181815 ... -0.0001133  -0.00266303\n",
      "   0.00048118]\n",
      " [-0.00125661  0.00181815 -0.00874294 ... -0.00029018  0.00325746\n",
      "  -0.00185258]\n",
      " ...\n",
      " [-0.00229909 -0.0001133  -0.00029018 ... -0.00855806  0.00229537\n",
      "  -0.00127144]\n",
      " [ 0.00443764 -0.00266303  0.00325746 ...  0.00229537 -0.00889308\n",
      "   0.00231061]\n",
      " [-0.00065676  0.00048118 -0.00185258 ... -0.00127144  0.00231061\n",
      "  -0.00599534]]\n",
      "\n",
      "Ridge Summary:\n",
      "beta_hat: [-7.01463766e-01 -4.14927788e-02  4.85240430e-01  9.87668094e-01\n",
      "  1.25040951e-01 -2.72393501e-01  1.67967585e-01 -3.99480101e-01\n",
      "  6.84333398e-01 -2.69287346e-01 -6.01096888e-01 -4.11982277e-01\n",
      "  2.14168569e-01  4.03590275e-01 -4.27594067e-01  3.51180459e-01\n",
      " -5.16279513e-01 -4.40772801e-01  5.05087869e-01  2.94026386e-01\n",
      "  3.24829802e-03 -1.52542973e-02 -4.43588610e-04 -5.67087078e-01\n",
      "  1.15890212e+00  4.72319497e-01 -1.66810151e-01 -3.94074104e-01\n",
      " -8.92116390e-01  5.99992524e-01  2.08375565e-01  1.48485484e-01\n",
      "  1.10248471e+00 -3.35140429e-01 -3.73342078e-01 -6.93918498e-01\n",
      "  5.26882859e-01  1.59862243e-01  8.46800777e-01  7.58725114e-01\n",
      " -4.00269462e-01 -5.16951742e-01  5.40614982e-01  3.29460454e-02\n",
      "  8.26647902e-01 -1.42721465e+00 -1.66923795e-01 -4.22893444e-01\n",
      " -1.75686068e-01 -8.72660402e-01  3.24860890e-01]\n",
      "R-squared: 0.9988452761966243\n"
     ]
    }
   ],
   "source": [
    "# Ridge Example\n",
    "ridge_model = Ridge(alpha=1.0, include_intercept=True)\n",
    "ridge_model.fit(X, y)\n",
    "ridge_variance = ridge_model.estimate_variance(X, y)\n",
    "print(\"Ridge Coefficients:\", ridge_model.beta)\n",
    "print(\"Ridge Variance Matrix:\\n\", ridge_variance)\n",
    "\n",
    "ridge_summary = summary(ridge_model, X, y)\n",
    "print(\"\\nRidge Summary:\")\n",
    "print(f\"beta_hat: {ridge_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ridge_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Matrix is ill-conditioned. Consider using regularization.\n",
      "OLS Variance Diagonal:\n",
      " [ 2.02693108e+18  1.68225086e+19 -2.92151166e+19  5.14237968e+18\n",
      "  8.36548935e+19  3.32472211e+19  6.37422787e+18 -2.40061949e+19\n",
      "  2.83345633e+18  3.82356111e+19  3.55652363e+19  1.64719905e+19\n",
      "  1.93644852e+19  6.60045095e+18  8.72882840e+18  2.54233661e+19\n",
      "  2.90486091e+19 -1.28468828e+19  1.29936203e+19 -1.11562766e+19\n",
      " -9.97898083e+17  1.09545341e+18 -1.28074517e+18  4.20234720e+19\n",
      "  8.87987347e+18 -7.11413639e+17  1.90566183e+18  1.82505815e+19\n",
      "  1.88134349e+19  2.02329430e+19 -1.46163524e+19 -3.67171415e+19\n",
      " -2.50699411e+19 -1.16401384e+19  5.28066926e+17 -6.91247057e+18\n",
      "  1.59585028e+20 -3.00431378e+19 -4.52530670e+18 -1.99863685e+19\n",
      "  5.23216667e+19  8.20666588e+19  3.38489940e+19 -1.12866407e+18\n",
      " -3.19395528e+18  3.73142090e+18  1.47646806e+19 -2.28193317e+19\n",
      " -1.05743676e+18  5.34038106e+19]\n",
      "Warning: Variance estimator assumes homoskedasticity.\n",
      "Ridge Variance Diagonal:\n",
      " [-0.0042696  -0.00372507 -0.0019232  -0.0033149  -0.00189268 -0.00519255\n",
      " -0.00678748 -0.00376899 -0.00235267 -0.00355763 -0.00380631 -0.00530575\n",
      " -0.00406727 -0.00332367 -0.00270397 -0.00386035 -0.00549554 -0.00270237\n",
      " -0.00271689 -0.00642804 -0.00565929 -0.00444493 -0.00297715 -0.00414961\n",
      " -0.00473143 -0.003752   -0.0047796  -0.00410062 -0.00279259 -0.00176272\n",
      " -0.00578667 -0.00392868 -0.00227724 -0.00507449 -0.00533864 -0.00527952\n",
      " -0.00239952 -0.00607736 -0.00283476 -0.0045888  -0.00219995 -0.00655785\n",
      " -0.00442928 -0.00282656 -0.00226813 -0.0030984  -0.00425674 -0.00424845\n",
      " -0.00264526 -0.00216689]\n",
      "\n",
      "OLS Summary:\n",
      "beta_hat: [  190.31431947  -647.96931744   187.46743567 -1078.48040696\n",
      " -1239.27156365    21.29730603  -461.10507324 -1188.71770334\n",
      "    74.24711187   -15.35687984  -318.48897449  -268.95821017\n",
      "  -207.09442029  -171.41599083 -1542.14098206  1268.60203335\n",
      "  -113.78322754  -404.37684995  -669.70406455  1744.53103788\n",
      "   200.07435454    -7.87835446    -7.07548922    17.69194587\n",
      "    89.06003286    33.10255934     2.55827332   -24.34555565\n",
      "    49.934185      21.34619559    19.39939211   -10.90047524\n",
      "   -17.1411611    -23.20816868   -25.08070407    21.16404604\n",
      "    33.73447132    -8.97682864    16.10612093    31.04205377\n",
      "    -9.88059047   -20.73159131    17.04924662   -29.91194824\n",
      "    28.25116375   -29.97236164    -4.30702139     2.08965999\n",
      "   -32.17329998    25.52470459     7.79854756]\n",
      "R-squared: -329.24800935734214\n",
      "\n",
      "Ridge Summary:\n",
      "beta_hat: [-1.45360987  0.14842268  0.66316335  0.11488005  0.02798998 -0.03062394\n",
      " -0.01459588 -0.09373551  0.11605848 -0.00787723  0.2117232   1.08743923\n",
      " -0.16716832 -0.63070889 -0.20126264  0.10271133  0.57900106 -0.35606291\n",
      "  0.28960279 -0.07034579  0.11097438 -0.1974567   0.20167649 -1.02969359\n",
      "  0.8717075   0.26249866 -0.68952325 -0.56064286 -0.49780237  0.48115862\n",
      " -0.47489613 -0.14458598 -0.32413079 -0.22095618  0.53821193  0.04915251\n",
      " -0.92752707  0.22434453  0.04706783  0.63062214  0.68233717  0.03050089\n",
      " -0.78200747  0.03438619  1.06338112 -0.17298955 -0.26628805  0.2597551\n",
      "  1.34662095 -0.64184959 -0.9431612 ]\n",
      "R-squared: 0.9992758595950716\n"
     ]
    }
   ],
   "source": [
    "# Now lets generate data with p > n and see how Ridge performs compared to OLS\n",
    "\n",
    "# We can see that the variance estimates of the OLS are extremely large, indicating the solution is unstable\n",
    "# Ridge on the other hand, provides a more stable solution.\n",
    "\n",
    "n_samples, n_features = 20, 50\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_beta = np.random.randn(n_features)\n",
    "y = X @ true_beta + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "ols_model = OLS(include_intercept=True)\n",
    "ols_model.fit(X, y)\n",
    "ols_predictions = ols_model.predict(X)\n",
    "ols_variance = ols_model.estimate_variance(X, y)\n",
    "print(\"OLS Variance Diagonal:\\n\", np.diag(ols_variance))\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0, include_intercept=True)\n",
    "ridge_model.fit(X, y)\n",
    "ridge_variance = ridge_model.estimate_variance(X, y)\n",
    "print(\"Ridge Variance Diagonal:\\n\", np.diag(ridge_variance))\n",
    "\n",
    "# Compare summaries\n",
    "ols_summary = summary(ols_model, X, y)\n",
    "print(\"\\nOLS Summary:\")\n",
    "print(f\"beta_hat: {ols_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ols_summary['r_squared']}\")\n",
    "ridge_summary = summary(ridge_model, X, y)\n",
    "print(\"\\nRidge Summary:\")\n",
    "print(f\"beta_hat: {ridge_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ridge_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Time: 154.03011989593506\n",
      "Converged after 335 iterations\n",
      "GD Time: 69.05917716026306\n"
     ]
    }
   ],
   "source": [
    "# Now lets compare the time taken to fit the models with a large number of features\n",
    "\n",
    "n_samples, n_features = 20000, 10000\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_beta = np.random.randn(n_features)\n",
    "y = X @ true_beta + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "start = time.time()\n",
    "ols_model = OLS(include_intercept=True)\n",
    "ols_model.fit(X, y)\n",
    "ols_predictions = ols_model.predict(X)\n",
    "end = time.time()\n",
    "print(\"OLS Time:\", end - start)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "gd_model = OLS(include_intercept=True)\n",
    "gd_model.fit(X, y, use_gradient_descent=True, max_iter=1000, alpha=0.1, tol=1e-6)\n",
    "gd_predictions = gd_model.predict(X)\n",
    "end = time.time()\n",
    "print(\"GD Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss_estimation.py\n",
    "\n",
    "This file provides functions to calculate different types of loss estimators for a given model over a dataset. These include the naive loss, training/testing loss, and leave-one-out loss estimators. The functions can be used with any model that implements the `fit()` and `predict()` methods.\n",
    "\n",
    "##### `naive_loss_estimation(model, X, y)`\n",
    "Calculates the naive loss estimator for a given model over a given dataset.\n",
    "\n",
    "##### Parameters:\n",
    "- **model** (`object`): The model for which the naive loss will be calculated. The model must have `fit()` and `predict()` methods implemented.\n",
    "- **X** (`numpy.ndarray`): A 2D array of shape `(n_samples, n_features)` representing the feature set of the dataset.\n",
    "- **y** (`numpy.ndarray`): A 1D array of length `n_samples` representing the true response values corresponding to the features in `X`.\n",
    "\n",
    "##### Returns:\n",
    "- **naive_loss_estimate** (`float`): The naive loss estimate, calculated as the mean squared error (MSE) between the true and predicted values.\n",
    "\n",
    "\n",
    "##### `train_test_loss_estimation(model, X, y, train_range, test_range)`\n",
    "This function calculates the training/testing loss estimator for a given model over a dataset, using a training set and a test set. The model is trained on the training set and evaluated on the test set. The loss is calculated as the Mean Squared Error (MSE) between the predicted and actual responses on the test set.\n",
    "\n",
    "##### Parameters:\n",
    "- **model** (`object`): The model for which the naive loss will be calculated. The model must have `fit()` and `predict()` methods implemented.\n",
    "- **X** (`numpy.ndarray`): A 2D array of shape `(n_samples, n_features)` representing the feature set of the dataset.\n",
    "- **y** (`numpy.ndarray`): A 1D array of length `n_samples` representing the true response values corresponding to the features in `X`.\n",
    "- **train_range** (`list`): The list of indices which will be used to train the model.\n",
    "- **test_range** (`list`): The list of indices which will the MSE of the trained model will be calculated on.\n",
    "\n",
    "##### Returns:\n",
    "- **train_test_loss_estimate** (`float`): The training-testing loss estimate, calculated as the mean squared error (MSE) between the true and predicted values over the testing data-set using the model trained on the training data set.\n",
    "\n",
    "\n",
    "\n",
    "##### `loss_test_loss_estimation(model, X, y))`\n",
    "Calculates the leave-one-out (LOO) loss estimator for a given model over a given dataset. Assumes that the model is a linear model and utilizes the known closed form solution for the LOO loss estimator for linear models for computational efficiency.\n",
    "\n",
    "##### Parameters:\n",
    "- **model** (`object`): The model for which the naive loss will be calculated. The model must have `fit()` and `predict()` methods implemented.\n",
    "- **X** (`numpy.ndarray`): A 2D array of shape `(n_samples, n_features)` representing the feature set of the dataset.\n",
    "- **y** (`numpy.ndarray`): A 1D array of length `n_samples` representing the true response values corresponding to the features in `X`.\n",
    "\n",
    "##### Returns:\n",
    "- **loo_loss_estimate** (`float`): The leave-one-out loss estimate, calculated using the closed form solution which is known for linear models.\n",
    "\n",
    "---\n",
    "### Example\n",
    "Below is a code snippet where the three loss-estimations are used in practice for an OLS-estimator using a generated data set. In the first and second outputs we showcase that when given the entire data set for training and testing, the training-testing loss estimator reduces to the naive loss estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate data:\n",
    "- For $n=1000$ samples and $p = 10$ covariates, generate design matrix X as standard normal data.\n",
    "- Generate $y = X\\beta + e$, where $\\beta$ is given and $e$ is from a standard normal distribution.\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat: [0.05829502 0.95225026 1.9562936  3.05512177 3.97184667 5.06638368\n",
      " 5.99138847 6.95918907 8.02403438 8.92732553 9.95198454]\n",
      "R-squared: 0.9972319710008215\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n = 1000\n",
    "p = 10\n",
    "X = np.random.randn(n,p)\n",
    "beta = np.arange(1,p+1)\n",
    "e = np.random.randn(n)\n",
    "y = X @  beta + e\n",
    "\n",
    "model = OLS(include_intercept=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "summ = summary(model, X,y)\n",
    "print(f\"beta_hat: {summ['coefficients']}\")\n",
    "print(f\"R-squared: {summ['r_squared']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive:\t\t\t0.9807422146110315\n",
      "Train-Test (full/full):\t0.9816713360206676\n",
      "Train-Test (half/half)):1.0587698658562517\n",
      "Leave-one-out:\t\t0.031690272476735934\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive:\\t\\t\\t\" +              str(naive_loss_estimation(model,X,y)))\n",
    "print(\"Train-Test (full/full):\\t\" + str(train_test_loss_estimation(model, X, y, list(range(1,1000)), list(range(1,1000)) )))\n",
    "print(\"Train-Test (half/half)):\" +  str(train_test_loss_estimation(model, X, y, list(range(1,500)), list(range(500,1000)) )))\n",
    "print(\"Leave-one-out:\\t\\t\" +        str(loo_loss_estimation(model, X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearModelTester.py\n",
    "\n",
    "The file holds a class for performing hypothesis tests and building confidence intervals on a fitted gaussian homoscedastic linear model.\n",
    "\n",
    "### Class: LinearModelTester(model)\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **model** (`object`): A fitted linear model object with:\n",
    "    - **$\\beta$** (`numpy.ndarray`): Estimated coefficients of the model.\n",
    "    - **include_intercept** (`bool`): Whether the model includes an intercept term.\n",
    "##### Raises:\n",
    "- ValueError: If the model is not fitted (**\\beta** is None).\n",
    "\n",
    "\n",
    "#### Methods:\n",
    "##### `hypothesis_t_test(X, y, null_hypothesis, alpha=0.05)`:\n",
    "Perform a t-test for individual coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **null_hypothesis** (`numpy.ndarray`): Hypothesized values of coefficients.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "###### Returns:\n",
    "- List of dictionaries with:\n",
    "    - **coefficient** (`int`): Index of the coefficient.\n",
    "    - **beta_estimate** (`numpy.ndarray`): Estimated value.\n",
    "    - **null_value** (`float`): Null hypothesis value for the coefficient.\n",
    "    - **t_stat** (`float`): T-statistic.\n",
    "    - **p_value** (`float`): P-value for t-statistic at significance level $\\alpha$.\n",
    "    - **reject_null** (`bool`): Whether the null hypothesis is rejected.\n",
    "\n",
    "##### `hypothesis_F_test(X, y, R, r, alpha=0.05)`:\n",
    "Perform an F-test for hypotheses of the form $R\\beta = r$.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **R** (`numpy.ndarray`): Constraint matrix $(k x p)$.\n",
    "- **r** (`numpy.ndarray`): Constraint vector $(k x 1)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "###### Returns:\n",
    "- Dictionary with:\n",
    "    - **F_stat** (`float`): F-statistic.\n",
    "    - **p_value** (`float`): P-value for F-statistic at significance level $\\alpha$.\n",
    "    - **reject_null** (`bool`): Whether the null hypothesis is rejected.\n",
    "\n",
    "##### `confidence_interval(X, y, alpha=0.05)`:\n",
    "Construct confidence intervals for model coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "###### Returns:\n",
    "- List of dictionaries with:\n",
    "    - **coefficient** (`int`): Index of the coefficient.\n",
    "    - **beta_estimate** (`float`): Estimated value of the coefficient.\n",
    "    - **confidence_lower** (`float`): Lower bound of the $1-\\alpha$ confidence interval.\n",
    "    - **confidence_upper** (`float`): Upper bound of the $1-\\alpha$ confidence interval.\n",
    "\n",
    "##### `prediction_interval_m(X, y, x_new, alpha=0.05)`:\n",
    "Construct a confidence interval for $m(x_{new}) = x_{new}^\\top\\beta$ at a new point ($x_{new}$).\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **x_new** (`numpy.ndarray`): New feature vector $(1 x p)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "##### Returns:\n",
    "- Dictionary with:\n",
    "    - **mx_new_estimate** (`np.ndarray`): Estimated $m(x_{new})$.\n",
    "    - **confidence_lower** (`float`): Lower bound of the $1-\\alpha$ confidence interval.\n",
    "    - **confidence_upper** (`float`): Upper bound of the $1-\\alpha$ confidence interval.\n",
    "\n",
    "##### `prediction_interval_y(X, y, x_new, alpha=0.05)`:\n",
    "Construct a confidence interval for a new observation, $y_{new}$.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **x_new** (`numpy.ndarray`): New feature vector $(1 x p)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "##### Returns\n",
    "- Dictionary with:\n",
    "    - **mx_new_estimate** (`np.ndarray`): Estimated $m(x_{new})$.\n",
    "    - **confidence_lower** (`float`): Lower bound of the $1-\\alpha$ confidence interval for $y_{new}$.\n",
    "    - **confidence_upper** (`float`): Upper bound of the $1-\\alpha$ confidence interval for $y_{new}$.\n",
    "\n",
    "##### `model_selection(X, y, models, criterion='naive')`\n",
    "Perform model selection based on a specified criterion.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix.\n",
    "- **y** (`numpy.ndarray`): Response vector.\n",
    "- **models** (`list`): A list of model instances to be evaluated.\n",
    "- **criterion** (`str`, default='naive'): The criterion for model selection. Options are 'naive', 'train_test', 'loo', .\n",
    "###### Returns:\n",
    "- **best_model** (`object`): The model instance that performs the best based on the specified criterion.\n",
    "\n",
    "##### `nested_model_selection_f_test(X, y, full_model, reduced_model, alpha=0.05)`\n",
    "Perform nested model selection F-test. Null hypothesis is that the reduced model is correct (sufficient).\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix.\n",
    "- **y** (`numpy.ndarray`): Response vector.\n",
    "- **full_model** (`object`): Full model instance using all covariates.\n",
    "- **reduced_model** (`object`): Reduced model instance using only some covariates.\n",
    "- **alpha** (`float`, default=0.05): Significance level of the test.\n",
    "###### Returns:\n",
    "- **result** (`dict`): Dictionary containing the F-statistic, p-value, and whether to reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat: [-0.02095641  0.98620133  1.01318309 -0.03289706  3.00847295 -1.02679092]\n",
      "R-squared: 0.9259204134491424\n"
     ]
    }
   ],
   "source": [
    "#generate a random dataset\n",
    "np.random.seed(0)\n",
    "n = 1000\n",
    "p = 5\n",
    "X = np.random.randn(n,p)\n",
    "beta = np.array([1,1,0,3,-1])\n",
    "e = np.random.randn(n)\n",
    "y = X @  beta + e\n",
    "\n",
    "#fit an OLS estimator\n",
    "model = OLS(include_intercept=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "#generate new point to predict\n",
    "x_new = np.random.randn(1, p)\n",
    "\n",
    "summ = summary(model, X,y)\n",
    "print(f\"beta_hat: {summ['coefficients']}\")\n",
    "print(f\"R-squared: {summ['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient 0:\n",
      "  Estimated: -0.020956408714166208\n",
      "  Null value: 0\n",
      "  t-stat: -0.6704950528825446\n",
      "  p-value: 0.5026980078679999\n",
      "  Reject null: False\n",
      "Coefficient 1:\n",
      "  Estimated: 0.9862013324637019\n",
      "  Null value: 1\n",
      "  t-stat: -0.43719825879662916\n",
      "  p-value: 0.6620625378445069\n",
      "  Reject null: False\n",
      "Coefficient 2:\n",
      "  Estimated: 1.013183086563438\n",
      "  Null value: 1\n",
      "  t-stat: 0.3953575243015688\n",
      "  p-value: 0.6926638823957914\n",
      "  Reject null: False\n",
      "Coefficient 3:\n",
      "  Estimated: -0.0328970595911511\n",
      "  Null value: 0\n",
      "  t-stat: -1.060481138694913\n",
      "  p-value: 0.28918337260347227\n",
      "  Reject null: False\n",
      "Coefficient 4:\n",
      "  Estimated: 3.0084729523193987\n",
      "  Null value: 3\n",
      "  t-stat: 0.2723490663792494\n",
      "  p-value: 0.7854101946438525\n",
      "  Reject null: False\n",
      "Coefficient 5:\n",
      "  Estimated: -1.026790915553867\n",
      "  Null value: -1\n",
      "  t-stat: -0.8484471213816155\n",
      "  p-value: 0.3963932771608605\n",
      "  Reject null: False\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 0:\n",
      "  Estimated: -0.020956408714166208\n",
      "  Confidence interval: [-0.08229001653158752, 0.04037719910325511]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 1:\n",
      "  Estimated: 0.9862013324637019\n",
      "  Confidence interval: [0.9242663588267555, 1.0481363061006483]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 2:\n",
      "  Estimated: 1.013183086563438\n",
      "  Confidence interval: [0.9477489576897662, 1.0786172154371096]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 3:\n",
      "  Estimated: -0.0328970595911511\n",
      "  Confidence interval: [-0.09377099267799052, 0.02797687349568833]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 4:\n",
      "  Estimated: 3.0084729523193987\n",
      "  Confidence interval: [2.94742289143181, 3.0695230132069873]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 5:\n",
      "  Estimated: -1.026790915553867\n",
      "  Confidence interval: [-1.0887549953747322, -0.9648268357330019]\n",
      "--------------------------------------------------------------------------------\n",
      "F-stat: 0.41268836220298705\n",
      "p-value: 0.6619818652712848\n",
      "Reject null: False\n"
     ]
    }
   ],
   "source": [
    "#hypothesis testing on the coefficients\n",
    "\n",
    "H0 = np.array([0,1,1,0,3,-1])\n",
    "alpha = 0.05\n",
    "\n",
    "# test H0 for each coefficient\n",
    "\n",
    "results = hypothesis_t_test(model,X, y, H0, alpha)\n",
    "for result in results:\n",
    "    print(f\"Coefficient {result['coefficient']}:\")\n",
    "    print(f\"  Estimated: {result['beta_estimate']}\")\n",
    "    print(f\"  Null value: {result['null_value']}\")\n",
    "    print(f\"  t-stat: {result['t_stat']}\")\n",
    "    print(f\"  p-value: {result['p_value']}\")\n",
    "    print(f\"  Reject null: {result['reject_null']}\")\n",
    "\n",
    "# build confidence intervals for coefficients\n",
    "results = confidence_interval(model,X, y, alpha)\n",
    "for result in results:\n",
    "    print('--'*40)\n",
    "    print(f\"Coefficient {result['coefficient']}:\")\n",
    "    print(f\"  Estimated: {result['beta_estimate']}\")\n",
    "    print(f\"  Confidence interval: [{result['confidence_lower']}, {result['confidence_upper']}]\")\n",
    "\n",
    "# hypothesis testing on linear combinations of coefficients\n",
    "R = np.array([\n",
    "    [0, 1, -1, 0, 0, 0],  # beta_1 - beta_2 = 0\n",
    "    [0, 0, 2, 0, 0, 2]   # 2*beta_2 + 2*beta_5 = 0\n",
    "])\n",
    "r = [0, 0] \n",
    "\n",
    "# H0 = Rbeta = r\n",
    "\n",
    "results = hypothesis_F_test(model,X, y, R, r, alpha)\n",
    "print('--'*40)\n",
    "print(f\"F-stat: {results['F_stat']}\")\n",
    "print(f\"p-value: {results['p_value']}\")\n",
    "print(f\"Reject null: {results['reject_null']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Prediction interval for new point m[[ 2.04253623 -0.91946118  0.11467003 -0.1374237   1.36552692]]:\n",
      "  Estimated m(x_new): -0.757505417301134\n",
      "  Confidence interval: [-0.9397209890558961, -0.5752898455463719]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction interval for response of new point, y_new:\n",
      "  Confidence interval: [-2.6998488506853864, 1.1848380160831187]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "result = prediction_interval_m(model, X, y, x_new, alpha)\n",
    "print('--'*40)\n",
    "print(f\"Prediction interval for new point m{x_new}:\")\n",
    "print(f\"  Estimated m(x_new): {result['mx_new_estimate']}\")\n",
    "print(f\"  Confidence interval: [{result['confidence_lower']}, {result['confidence_upper']}]\")\n",
    "\n",
    "result = prediction_interval_y(model, X, y, x_new, alpha)\n",
    "print('--'*40)\n",
    "print(f\"Prediction interval for response of new point, y_new:\")\n",
    "print(f\"  Confidence interval: [{result['confidence_lower']}, {result['confidence_upper']}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Best model (naive criterion): OLS\n",
      "Best model (train_test criterion): OLS\n",
      "Best model (loo criterion): OLS\n",
      "--------------------------------------------------------------------------------\n",
      "Best model (naive criterion with heteroskedasticity): OLS\n",
      "Best model (train_test criterion with heteroskedasticity): OLS\n",
      "Best model (loo criterion with heteroskedasticity): OLS\n",
      "--------------------------------------------------------------------------------\n",
      "Warning: Matrix is ill-conditioned. Consider using regularization.\n",
      "Best model (naive criterion with multicollinearity): Ridge\n",
      "Warning: Matrix is ill-conditioned. Consider using regularization.\n",
      "Best model (train_test criterion with multicollinearity): Ridge\n",
      "Best model (loo criterion with multicollinearity): OLS\n"
     ]
    }
   ],
   "source": [
    "#generate a random dataset\n",
    "np.random.seed(0)\n",
    "n = 10000\n",
    "p = 10\n",
    "X = np.random.randn(n,p)\n",
    "beta = np.array(np.random.randn(p))\n",
    "e = np.random.randn(n)\n",
    "y = X @  beta + e\n",
    "\n",
    "ols_model = OLS(include_intercept=True)\n",
    "gls_model = GLS(include_intercept=True)\n",
    "ridge_model = Ridge(alpha=1, include_intercept=True)\n",
    "ols_model.fit(X, y)\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "\n",
    "print('--'*40)\n",
    "# Perform model selection using naive criterion\n",
    "best_model_naive = model_selection(X, y, [ols_model, ridge_model], criterion='naive')\n",
    "print(\"Best model (naive criterion):\", type(best_model_naive).__name__)\n",
    "# Perform model selection using train_test criterion\n",
    "best_model_train_test = model_selection(X, y, [ols_model, ridge_model], criterion='train_test')\n",
    "print(\"Best model (train_test criterion):\", type(best_model_train_test).__name__)\n",
    "# Perform model selection using loo criterion\n",
    "best_model_loo = model_selection(X, y, [ols_model, ridge_model], criterion='loo')\n",
    "print(\"Best model (loo criterion):\", type(best_model_loo).__name__)\n",
    "\n",
    "print('--'*40)\n",
    "heteroscedastic_y = X@beta + np.random.normal(0, np.linspace(1, 10, n))\n",
    "\n",
    "gls_model.fit(X, heteroscedastic_y, sigma=np.diag(np.linspace(1, 10, n)))\n",
    "ols_model.fit(X, heteroscedastic_y)\n",
    "ridge_model.fit(X, heteroscedastic_y)\n",
    "\n",
    "# Perform model selection using naive criterion with heteroskedasticity\n",
    "best_model_naive_heteroskedastic = model_selection(X, heteroscedastic_y, [ols_model, gls_model, ridge_model], criterion='naive')\n",
    "print(\"Best model (naive criterion with heteroskedasticity):\", type(best_model_naive_heteroskedastic).__name__)\n",
    "#Perform model selection using train_test criterion with heteroskedasticity\n",
    "best_model_heteroskedastic =model_selection(X, heteroscedastic_y, [ols_model, gls_model, ridge_model], criterion='train_test')\n",
    "print(\"Best model (train_test criterion with heteroskedasticity):\", type(best_model_heteroskedastic).__name__)\n",
    "# Perform model selection using loo criterion with heteroskedasticity\n",
    "print(\"Best model (loo criterion with heteroskedasticity):\", type(model_selection(X, heteroscedastic_y, [ols_model, gls_model, ridge_model], criterion='loo')).__name__)\n",
    "\n",
    "print('--'*40)\n",
    "# Perform model selection using train_test criterion with multicollinearity\n",
    "n = 1000\n",
    "p = 2000\n",
    "X_multicollinear = np.random.randn(n,p)\n",
    "beta = np.array(np.random.randn(p))\n",
    "e = np.random.randn(n)\n",
    "y_multicollinear = X_multicollinear @  beta + e\n",
    "\n",
    "ols_model.fit(X_multicollinear, y_multicollinear)\n",
    "gls_model.fit(X_multicollinear, y_multicollinear, sigma=np.eye(n))\n",
    "ridge_model.fit(X_multicollinear, y_multicollinear)\n",
    "\n",
    "# Perform model selection using naive criterion with multicollinearity\n",
    "best_model_multicollinear = model_selection(X_multicollinear, y_multicollinear, [ols_model, gls_model, ridge_model], criterion='naive')\n",
    "print(\"Best model (naive criterion with multicollinearity):\", type(best_model_multicollinear).__name__)\n",
    "# Perform model selection using train_test criterion with multicollinearity\n",
    "best_model_multicollinear = model_selection(X_multicollinear, y_multicollinear, [ols_model, gls_model, ridge_model], criterion='train_test')\n",
    "print(\"Best model (train_test criterion with multicollinearity):\", type(best_model_multicollinear).__name__)\n",
    "# Perform model selection using loo criterion with multicollinearity\n",
    "best_model_multicollinear = model_selection(X_multicollinear, y_multicollinear, [ols_model, gls_model, ridge_model], criterion='loo')\n",
    "print(\"Best model (loo criterion with multicollinearity):\", type(best_model_multicollinear).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Testing null hypothesis that reduced_model_1 is sufficient:\n",
      " F-stat: -0.0\n",
      " p-value: nan\n",
      " Reject null hypothesis: False\n",
      "--------------------------------------------------------------------------------\n",
      "Testing null hypothesis that reduced_model_2 is sufficient:\n",
      " F-stat: 26144.53702055332\n",
      " p-value: 1.1102230246251565e-16\n",
      " Reject null hypothesis: True\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 1000, 5\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_beta = np.array([1, 2, 3, 4, 5])\n",
    "y = X @ true_beta + np.random.randn(n_samples)\n",
    "\n",
    "full_model = OLS(include_intercept=True)\n",
    "full_model.fit(X, y)\n",
    "\n",
    "reduced_model_1 = ReducedModel(full_model, selected_features=[0, 1, 2,3,4])\n",
    "reduced_model_2 = ReducedModel(full_model, selected_features=[0, 1])\n",
    "\n",
    "\n",
    "result_1 = nested_model_selection_f_test(X, y, full_model, reduced_model_1, alpha=0.05)\n",
    "print('--'*40)\n",
    "print(\"Testing null hypothesis that reduced_model_1 is sufficient:\")\n",
    "print(f\" F-stat: {result_1['F_stat']}\")\n",
    "print(f\" p-value: {result_1['p_value']}\")\n",
    "print(f\" Reject null hypothesis: {result_1['reject_null']}\")\n",
    "\n",
    "result_2 = nested_model_selection_f_test(X, y, full_model, reduced_model_2, alpha=0.05)\n",
    "print('--'*40)\n",
    "print(\"Testing null hypothesis that reduced_model_2 is sufficient:\")\n",
    "print(f\" F-stat: {result_2['F_stat']}\")\n",
    "print(f\" p-value: {result_2['p_value']}\")\n",
    "print(f\" Reject null hypothesis: {result_2['reject_null']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
