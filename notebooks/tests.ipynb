{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Module Documentation\n",
    "MATH 533 Assignment 4\n",
    "\n",
    "December 20th, 2024\n",
    "\n",
    "Gulliver Hager, Evelyn Hubbard, Michael Montemurri\n",
    "\n",
    "---\n",
    "## models.py\n",
    "\n",
    "This file provides examples and documentation for the key methods implemented in the `OLS`, `GLS`, and `Ridge` regression models. The examples illustrate how to use these methods, including fitting models, making predictions, and obtaining variance estimates.\n",
    "\n",
    "### Class: OLS(include_intercept)\n",
    "Ordinary Least Squares (OLS) regression model.\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term, default = true.\n",
    "#### Methods:\n",
    "##### `OLS.fit(X, y, use_gradient_descent=False, max_iter=1000, alpha=0.01, tol=1e-6)`\n",
    "Fits the Ordinary Least Squares (OLS) model to the dataset.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "- **use_gradient_descent** (`bool`, optional): Whether to use gradient descent for optimization (default: False).\n",
    "- **max_iter** (`int`, optional): Maximum iterations for gradient descent (default: 1000).\n",
    "- **alpha** (`float`, optional): Learning rate for gradient descent (default: 0.01).\n",
    "- **tol** (`float`, optional): Convergence tolerance for gradient descent (default: 1e-6).\n",
    "###### Returns:\n",
    "None. Fits the model and stores coefficients in `self.beta`.\n",
    "\n",
    "##### `OLS.predict(X)`\n",
    "Predicts response values using the fitted OLS model.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "###### Returns:\n",
    "- **predictions** (`numpy.ndarray`): Predicted values for each sample.\n",
    "\n",
    "##### `OLS.estimate_variance(X, y)`\n",
    "Estimates the variance-covariance matrix of the coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "###### Returns:\n",
    "- **variance_matrix** (`numpy.ndarray`): Variance-covariance matrix of the coefficients.\n",
    "\n",
    "### Class: GLS(include_intercept)\n",
    "Generalized Least Squares (GLS) regression model.\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term, default = true.\n",
    "#### Methods:\n",
    "##### `GLS.fit(X, y, sigma)`\n",
    "Fits the Generalized Least Squares (GLS) model to the dataset.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "- **sigma** (`numpy.ndarray`): Covariance matrix of the errors.\n",
    "###### Returns:\n",
    "None. Fits the model and stores coefficients in `self.beta`.\n",
    "\n",
    "### Class: Ridge(lambda, include_intercept)\n",
    "Ridge regression model.\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **include_intercept** (`bool`): Whether the model includes an intercept term, default = true.\n",
    "- **lambda** (`float`): $\\lambda$ for the Ridge Estimator. \n",
    "#### Methods:\n",
    "##### `Ridge.fit(X, y)`\n",
    "Fits the Ridge regression model to the dataset.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "###### Returns:\n",
    "None. Fits the model and stores coefficients in `self.beta`.\n",
    "##### `Ridge.estimate_variance(X, y)`\n",
    "Estimates the variance-covariance matrix of the coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "###### Returns:\n",
    "- **variance_matrix** (`numpy.ndarray`): Variance-covariance matrix of the coefficients.\n",
    "\n",
    "### Function: summary(model,X,y)\n",
    "Generate a summary of the model's performance.\n",
    "#### Parameters:\n",
    "- **model** (`object`) : The regression model (OLS, GLS, or Ridge). Must have a `predict` method.\n",
    "- **X** (`numpy.ndarray` or `pandas.DataFrame`): Feature matrix of shape `(n_samples, n_features)`.\n",
    "- **y** (`numpy.ndarray` or `pandas.Series`): Response vector of length `n_samples`.\n",
    "\n",
    "#### Returns\n",
    "- A dictionary containing:\n",
    "    - **coefficients** (`numpy.ndarray`): The fitted coefficients of the model.\n",
    "    - **r_squared** (`float`): The R-squared value, representing the proportion of variance explained by the model.\n",
    "\n",
    "---\n",
    "## Example\n",
    "\n",
    "Below is an example demonstrating the use of `OLS`, `GLS`, and `Ridge` regression models using synthetic datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from stats_module import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Coefficients: [-0.14050914  0.31998031 -0.04676428  1.07625074 -0.2476699  -0.28304471]\n",
      "OLS Variance Matrix:\n",
      " [[ 0.26947461  0.01435536  0.02568034 -0.02309508  0.03055034]\n",
      " [ 0.01435536  0.30429335  0.03317391 -0.01444322 -0.03871695]\n",
      " [ 0.02568034  0.03317391  0.27349674 -0.00615378 -0.0117382 ]\n",
      " [-0.02309508 -0.01444322 -0.00615378  0.25447581 -0.02888467]\n",
      " [ 0.03055034 -0.03871695 -0.0117382  -0.02888467  0.29307544]]\n",
      "\n",
      "OLS Summary:\n",
      "beta_hat: [-0.14050914  0.31998031 -0.04676428  1.07625074 -0.2476699  -0.28304471]\n",
      "R-squared: 0.8405222067551491\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 5\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_beta = np.random.randn(n_features)\n",
    "y = X @ true_beta + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# OLS Example\n",
    "ols_model = OLS(include_intercept=True)\n",
    "ols_model.fit(X, y)\n",
    "ols_predictions = ols_model.predict(X)\n",
    "ols_variance = ols_model.estimate_variance(X, y)\n",
    "print(\"OLS Coefficients:\", ols_model.beta)\n",
    "print(\"OLS Variance Matrix:\\n\", ols_variance)\n",
    "\n",
    "#get summary\n",
    "ols_summary = summary(ols_model, X, y)\n",
    "print(\"\\nOLS Summary:\")\n",
    "print(f\"beta_hat: {ols_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ols_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 37 iterations\n",
      "GD Coefficients: [-0.14112852  0.31944985 -0.04772522  1.07540995 -0.24760737 -0.28247484]\n",
      "\n",
      "GD Summary:\n",
      "beta_hat: [-0.14112852  0.31944985 -0.04772522  1.07540995 -0.24760737 -0.28247484]\n",
      "R-squared: 0.840521026564396\n"
     ]
    }
   ],
   "source": [
    "# Now using the gradient descent method\n",
    "gd_model = OLS(include_intercept=True)\n",
    "gd_model.fit(X, y, use_gradient_descent=True, max_iter=1000, alpha=0.1, tol=1e-6)\n",
    "gd_predictions = gd_model.predict(X)\n",
    "gd_variance = gd_model.estimate_variance(X, y)\n",
    "print(\"GD Coefficients:\", gd_model.beta)\n",
    "\n",
    "gd_summary = summary(gd_model, X, y)\n",
    "print(\"\\nGD Summary:\")\n",
    "print(f\"beta_hat: {gd_summary['coefficients']}\")\n",
    "print(f\"R-squared: {gd_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLS Coefficients: [-0.16562031  0.30614536 -0.08014721  1.05472803 -0.23844273 -0.29863188]\n",
      "\n",
      "GLS Summary:\n",
      "beta_hat: [-0.16562031  0.30614536 -0.08014721  1.05472803 -0.23844273 -0.29863188]\n",
      "R-squared: 0.8390183874062362\n"
     ]
    }
   ],
   "source": [
    "# GLS Example\n",
    "sigma = np.diag(np.random.uniform(0.5, 1.5, size=n_samples))\n",
    "gls_model = GLS(include_intercept=True)\n",
    "gls_model.fit(X, y, sigma)\n",
    "gls_predictions = gls_model.predict(X)\n",
    "print(\"GLS Coefficients:\", gls_model.beta)\n",
    "\n",
    "gls_summary = summary(gls_model, X, y)\n",
    "print(\"\\nGLS Summary:\")\n",
    "print(f\"beta_hat: {gls_summary['coefficients']}\")\n",
    "print(f\"R-squared: {gls_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLS Coefficients: [-0.14050914  0.31998031 -0.04676428  1.07625074 -0.2476699  -0.28304471]\n",
      "OLS Coefficients: [-0.14050914  0.31998031 -0.04676428  1.07625074 -0.2476699  -0.28304471]\n"
     ]
    }
   ],
   "source": [
    "# We can see that this reduces to OLS when sigma is the identity matrix\n",
    "gls_model = GLS(include_intercept=True)\n",
    "gls_model.fit(X, y, np.eye(n_samples))\n",
    "gls_predictions = gls_model.predict(X)\n",
    "gls_variance = gls_model.estimate_variance(X, y)\n",
    "print(\"GLS Coefficients:\", gls_model.beta)\n",
    "print(\"OLS Coefficients:\", ols_model.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Variance estimator assumes homoskedasticity.\n",
      "Ridge Coefficients: [-0.14184602  0.31581555 -0.04829966  1.0648917  -0.24528437 -0.28000121]\n",
      "Ridge Variance Matrix:\n",
      " [[ 0.26413765  0.01376477  0.02466189 -0.02213821  0.02933168]\n",
      " [ 0.01376477  0.29747623  0.03173532 -0.01390604 -0.03709305]\n",
      " [ 0.02466189  0.03173532  0.26803154 -0.00586855 -0.0112387 ]\n",
      " [-0.02213821 -0.01390604 -0.00586855  0.24975367 -0.02774526]\n",
      " [ 0.02933168 -0.03709305 -0.0112387  -0.02774526  0.28671054]]\n",
      "\n",
      "Ridge Summary:\n",
      "beta_hat: [-0.14184602  0.31581555 -0.04829966  1.0648917  -0.24528437 -0.28000121]\n",
      "R-squared: 0.8404299157997424\n"
     ]
    }
   ],
   "source": [
    "# Ridge Example\n",
    "ridge_model = Ridge(alpha=1.0, include_intercept=True)\n",
    "ridge_model.fit(X, y)\n",
    "ridge_variance = ridge_model.estimate_variance(X, y)\n",
    "print(\"Ridge Coefficients:\", ridge_model.beta)\n",
    "print(\"Ridge Variance Matrix:\\n\", ridge_variance)\n",
    "\n",
    "ridge_summary = summary(ridge_model, X, y)\n",
    "print(\"\\nRidge Summary:\")\n",
    "print(f\"beta_hat: {ridge_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ridge_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Matrix is ill-conditioned. Consider using regularization.\n",
      "OLS Variance Diagonal:\n",
      " [-1.00027051e+18 -4.11561002e+17 -8.83232400e+17 -2.91526426e+18\n",
      "  1.45284392e+18  2.52272950e+17 -3.32767797e+18 -1.61982336e+18\n",
      "  4.95819154e+17 -2.09801678e+17 -1.35443681e+18 -2.35029943e+18\n",
      "  1.56118544e+18 -2.64321622e+18 -1.43048224e+18 -2.39659974e+16\n",
      "  9.80690340e+17 -2.35302264e+18  2.51868425e+18 -7.42779700e+16\n",
      " -1.17279348e+18  3.84987422e+18  1.24361756e+18  2.15220374e+18\n",
      " -2.97372635e+17 -2.26337646e+18  6.35799974e+17 -3.05704482e+17\n",
      " -2.63055648e+18  1.03956358e+18 -1.06065915e+16  2.43532805e+18\n",
      " -2.33321652e+16  2.17650193e+17  1.30112746e+18 -3.17725861e+18\n",
      "  5.21764722e+17  1.99748045e+17  2.71888113e+18  3.22311387e+18\n",
      "  3.79642224e+15 -3.80678816e+18 -1.05509397e+18  5.81193890e+17\n",
      " -4.44989705e+17 -3.95919953e+18  2.36747029e+18  1.04849552e+18\n",
      " -1.00743896e+19 -1.42364082e+18]\n",
      "Warning: Variance estimator assumes homoskedasticity.\n",
      "Ridge Variance Diagonal:\n",
      " [-0.00387943 -0.00331424 -0.00574934 -0.0065698  -0.00235589 -0.0034576\n",
      " -0.00370803 -0.00471214 -0.00650786 -0.00539348 -0.00317097 -0.00495568\n",
      " -0.0050946  -0.00654048 -0.00345177 -0.00347624 -0.00310427 -0.0039868\n",
      " -0.00276963 -0.00358387 -0.00391177 -0.00365975 -0.00643869 -0.00296956\n",
      " -0.0019417  -0.00269505 -0.00482714 -0.00304399 -0.00243537 -0.00429298\n",
      " -0.00326938 -0.00332661 -0.00374101 -0.00346655 -0.00321711 -0.00195383\n",
      " -0.00407653 -0.0066283  -0.00333123 -0.00423844 -0.00319587 -0.0030195\n",
      " -0.00350328 -0.00208771 -0.00240979 -0.00343103 -0.00273033 -0.00337909\n",
      " -0.00266603 -0.00279876]\n",
      "\n",
      "OLS Summary:\n",
      "beta_hat: [ 22.10015332 -38.63171691 -27.19514443 -20.37591834 -23.54465356\n",
      "  28.17158947   9.44410235   4.29741989 -17.12584048  52.59083681\n",
      "  -3.11333166  -7.00377512  -5.77654841 -32.90885859  14.9760985\n",
      " -12.71931573  -3.60228551  33.73175832 -22.39716504 -57.94860324\n",
      "  -0.57063535 -15.13510264  26.9555934    0.91623412 -40.77199387\n",
      " -10.59263792  17.85310039  -5.56617397   2.91110347  10.7890792\n",
      " -30.07381279  32.0721735  -16.20783875 -18.36912304 -13.25152709\n",
      "   2.668902    15.53075351  -7.38049717   7.64839982  -2.58374279\n",
      "   2.15204953 -36.566747    -7.85001233  28.51132579  -2.44166069\n",
      "  -0.43764314  -0.96623481   4.81615639 -11.63748813  -6.50964356\n",
      "  15.98145666]\n",
      "R-squared: -48.23944778115864\n",
      "\n",
      "Ridge Summary:\n",
      "beta_hat: [-1.34719497  0.77398079  0.73630112  0.42978655  0.49203589  0.82821854\n",
      " -0.15060446 -0.1814956  -0.06519755  1.08147042 -0.73274491 -0.11153067\n",
      " -0.17631056  0.08735057  0.29573005 -0.01476706 -0.44957106 -0.19286649\n",
      " -1.09231437  0.62381021  1.20741889 -0.27271723 -0.53914667 -0.37619093\n",
      " -0.74619543 -0.39372487  0.08177853  0.13896543  0.19514073  0.49869108\n",
      " -0.09565469 -0.05391762 -0.31826037 -0.10159886  0.07621272  0.47508852\n",
      " -0.53681794 -0.48186805 -0.2920847  -0.16140739  0.63522107  0.19150801\n",
      "  0.66062234  0.05986914 -0.11559253 -0.06891977  0.19174071  0.08619623\n",
      " -0.27343756 -0.67357751 -0.44418557]\n",
      "R-squared: 0.9988726980710212\n"
     ]
    }
   ],
   "source": [
    "# Now lets generate data with p > n and see how Ridge performs compared to OLS\n",
    "\n",
    "# We can see that the variance estimates of the OLS are extremely large, indicating the solution is unstable\n",
    "# Ridge on the other hand, provides a more stable solution.\n",
    "\n",
    "n_samples, n_features = 20, 50\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_beta = np.random.randn(n_features)\n",
    "y = X @ true_beta + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "ols_model = OLS(include_intercept=True)\n",
    "ols_model.fit(X, y)\n",
    "ols_predictions = ols_model.predict(X)\n",
    "ols_variance = ols_model.estimate_variance(X, y)\n",
    "print(\"OLS Variance Diagonal:\\n\", np.diag(ols_variance))\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0, include_intercept=True)\n",
    "ridge_model.fit(X, y)\n",
    "ridge_variance = ridge_model.estimate_variance(X, y)\n",
    "print(\"Ridge Variance Diagonal:\\n\", np.diag(ridge_variance))\n",
    "\n",
    "# Compare summaries\n",
    "ols_summary = summary(ols_model, X, y)\n",
    "print(\"\\nOLS Summary:\")\n",
    "print(f\"beta_hat: {ols_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ols_summary['r_squared']}\")\n",
    "ridge_summary = summary(ridge_model, X, y)\n",
    "print(\"\\nRidge Summary:\")\n",
    "print(f\"beta_hat: {ridge_summary['coefficients']}\")\n",
    "print(f\"R-squared: {ridge_summary['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Time: 151.44079184532166\n",
      "Converged after 336 iterations\n",
      "GD Time: 67.34271597862244\n"
     ]
    }
   ],
   "source": [
    "# Now lets compare the time taken to fit the models with a large number of features\n",
    "\n",
    "n_samples, n_features = 20000, 10000\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_beta = np.random.randn(n_features)\n",
    "y = X @ true_beta + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "start = time.time()\n",
    "ols_model = OLS(include_intercept=True)\n",
    "ols_model.fit(X, y)\n",
    "ols_predictions = ols_model.predict(X)\n",
    "end = time.time()\n",
    "print(\"OLS Time:\", end - start)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "gd_model = OLS(include_intercept=True)\n",
    "gd_model.fit(X, y, use_gradient_descent=True, max_iter=1000, alpha=0.1, tol=1e-6)\n",
    "gd_predictions = gd_model.predict(X)\n",
    "end = time.time()\n",
    "print(\"GD Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss_estimation.py\n",
    "\n",
    "This file provides functions to calculate different types of loss estimators for a given model over a dataset. These include the naive loss, training/testing loss, and leave-one-out loss estimators. The functions can be used with any model that implements the `fit()` and `predict()` methods.\n",
    "\n",
    "##### `naive_loss_estimation(model, X, y)`\n",
    "Calculates the naive loss estimator for a given model over a given dataset.\n",
    "\n",
    "##### Parameters:\n",
    "- **model** (`object`): The model for which the naive loss will be calculated. The model must have `fit()` and `predict()` methods implemented.\n",
    "- **X** (`numpy.ndarray`): A 2D array of shape `(n_samples, n_features)` representing the feature set of the dataset.\n",
    "- **y** (`numpy.ndarray`): A 1D array of length `n_samples` representing the true response values corresponding to the features in `X`.\n",
    "\n",
    "##### Returns:\n",
    "- **naive_loss_estimate** (`float`): The naive loss estimate, calculated as the mean squared error (MSE) between the true and predicted values.\n",
    "\n",
    "\n",
    "##### `train_test_loss_estimation(model, X, y, train_range, test_range)`\n",
    "This function calculates the training/testing loss estimator for a given model over a dataset, using a training set and a test set. The model is trained on the training set and evaluated on the test set. The loss is calculated as the Mean Squared Error (MSE) between the predicted and actual responses on the test set.\n",
    "\n",
    "##### Parameters:\n",
    "- **model** (`object`): The model for which the naive loss will be calculated. The model must have `fit()` and `predict()` methods implemented.\n",
    "- **X** (`numpy.ndarray`): A 2D array of shape `(n_samples, n_features)` representing the feature set of the dataset.\n",
    "- **y** (`numpy.ndarray`): A 1D array of length `n_samples` representing the true response values corresponding to the features in `X`.\n",
    "- **train_range** (`list`): The list of indices which will be used to train the model.\n",
    "- **test_range** (`list`): The list of indices which will the MSE of the trained model will be calculated on.\n",
    "\n",
    "##### Returns:\n",
    "- **train_test_loss_estimate** (`float`): The training-testing loss estimate, calculated as the mean squared error (MSE) between the true and predicted values over the testing data-set using the model trained on the training data set.\n",
    "\n",
    "\n",
    "\n",
    "##### `loss_test_loss_estimation(model, X, y))`\n",
    "Calculates the leave-one-out (LOO) loss estimator for a given model over a given dataset. Assumes that the model is a linear model and utilizes the known closed form solution for the LOO loss estimator for linear models for computational efficiency.\n",
    "\n",
    "##### Parameters:\n",
    "- **model** (`object`): The model for which the naive loss will be calculated. The model must have `fit()` and `predict()` methods implemented.\n",
    "- **X** (`numpy.ndarray`): A 2D array of shape `(n_samples, n_features)` representing the feature set of the dataset.\n",
    "- **y** (`numpy.ndarray`): A 1D array of length `n_samples` representing the true response values corresponding to the features in `X`.\n",
    "\n",
    "##### Returns:\n",
    "- **loo_loss_estimate** (`float`): The leave-one-out loss estimate, calculated using the closed form solution which is known for linear models.\n",
    "\n",
    "---\n",
    "### Example\n",
    "Below is a code snippet where the three loss-estimations are used in practice for an OLS-estimator using a generated data set. In the first and second outputs we showcase that when given the entire data set for training and testing, the training-testing loss estimator reduces to the naive loss estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate data:\n",
    "- For $n=1000$ samples and $p = 10$ covariates, generate design matrix X as standard normal data.\n",
    "- Generate $y = X\\beta + e$, where $\\beta$ is given and $e$ is from a standard normal distribution.\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat: [0.05829502 0.95225026 1.9562936  3.05512177 3.97184667 5.06638368\n",
      " 5.99138847 6.95918907 8.02403438 8.92732553 9.95198454]\n",
      "R-squared: 0.9972319710008215\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n = 1000\n",
    "p = 10\n",
    "X = np.random.randn(n,p)\n",
    "beta = np.arange(1,p+1)\n",
    "e = np.random.randn(n)\n",
    "y = X @  beta + e\n",
    "\n",
    "model = OLS(include_intercept=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "summ = summary(model, X,y)\n",
    "print(f\"beta_hat: {summ['coefficients']}\")\n",
    "print(f\"R-squared: {summ['r_squared']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive:\t\t\t0.9807422146110315\n",
      "Train-Test (full/full):\t0.9816713360206676\n",
      "Train-Test (half/half)):1.0587698658562517\n",
      "Leave-one-out:\t\t0.031690272476735934\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive:\\t\\t\\t\" +              str(naive_loss_estimation(model,X,y)))\n",
    "print(\"Train-Test (full/full):\\t\" + str(train_test_loss_estimation(model, X, y, list(range(1,1000)), list(range(1,1000)) )))\n",
    "print(\"Train-Test (half/half)):\" +  str(train_test_loss_estimation(model, X, y, list(range(1,500)), list(range(500,1000)) )))\n",
    "print(\"Leave-one-out:\\t\\t\" +        str(loo_loss_estimation(model, X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearModelTester.py\n",
    "\n",
    "The file holds a class for performing hypothesis tests and building confidence intervals on a fitted gaussian homoscedastic linear model.\n",
    "\n",
    "### Class: LinearModelTester(model)\n",
    "#### Initialization:\n",
    "##### Parameters:\n",
    "- **model** (`object`): A fitted linear model object with:\n",
    "    - **$\\beta$** (`numpy.ndarray`): Estimated coefficients of the model.\n",
    "    - **include_intercept** (`bool`): Whether the model includes an intercept term.\n",
    "##### Raises:\n",
    "- ValueError: If the model is not fitted (**\\beta** is None).\n",
    "\n",
    "\n",
    "#### Methods:\n",
    "##### `hypothesis_t_test(X, y, null_hypothesis, alpha=0.05)`:\n",
    "Perform a t-test for individual coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **null_hypothesis** (`numpy.ndarray`): Hypothesized values of coefficients.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "###### Returns:\n",
    "- List of dictionaries with:\n",
    "    - **coefficient** (`int`): Index of the coefficient.\n",
    "    - **beta_estimate** (`numpy.ndarray`): Estimated value.\n",
    "    - **null_value** (`float`): Null hypothesis value for the coefficient.\n",
    "    - **t_stat** (`float`): T-statistic.\n",
    "    - **p_value** (`float`): P-value for t-statistic at significance level $\\alpha$.\n",
    "    - **reject_null** (`bool`): Whether the null hypothesis is rejected.\n",
    "\n",
    "##### `hypothesis_F_test(X, y, R, r, alpha=0.05)`:\n",
    "Perform an F-test for hypotheses of the form $R\\beta = r$.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **R** (`numpy.ndarray`): Constraint matrix $(k x p)$.\n",
    "- **r** (`numpy.ndarray`): Constraint vector $(k x 1)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "###### Returns:\n",
    "- Dictionary with:\n",
    "    - **F_stat** (`float`): F-statistic.\n",
    "    - **p_value** (`float`): P-value for F-statistic at significance level $\\alpha$.\n",
    "    - **reject_null** (`bool`): Whether the null hypothesis is rejected.\n",
    "\n",
    "##### `confidence_interval(X, y, alpha=0.05)`:\n",
    "Construct confidence intervals for model coefficients.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "###### Returns:\n",
    "- List of dictionaries with:\n",
    "    - **coefficient** (`int`): Index of the coefficient.\n",
    "    - **beta_estimate** (`float`): Estimated value of the coefficient.\n",
    "    - **confidence_lower** (`float`): Lower bound of the $1-\\alpha$ confidence interval.\n",
    "    - **confidence_upper** (`float`): Upper bound of the $1-\\alpha$ confidence interval.\n",
    "\n",
    "##### `prediction_interval_m(X, y, x_new, alpha=0.05)`:\n",
    "Construct a confidence interval for $m(x_{new}) = x_{new}^\\top\\beta$ at a new point ($x_{new}$).\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **x_new** (`numpy.ndarray`): New feature vector $(1 x p)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "##### Returns:\n",
    "- Dictionary with:\n",
    "    - **mx_new_estimate** (`np.ndarray`): Estimated $m(x_{new})$.\n",
    "    - **confidence_lower** (`float`): Lower bound of the $1-\\alpha$ confidence interval.\n",
    "    - **confidence_upper** (`float`): Upper bound of the $1-\\alpha$ confidence interval.\n",
    "\n",
    "##### `prediction_interval_y(X, y, x_new, alpha=0.05)`:\n",
    "Construct a confidence interval for a new observation, $y_{new}$.\n",
    "###### Parameters:\n",
    "- **X** (`numpy.ndarray`): Feature matrix $(n x p)$.\n",
    "- **y** (`numpy.ndarray`): Response vector $(n x 1)$.\n",
    "- **x_new** (`numpy.ndarray`): New feature vector $(1 x p)$.\n",
    "- **$\\alpha$** (`float`): Significance level (default 0.05).\n",
    "##### Returns\n",
    "- Dictionary with:\n",
    "    - **mx_new_estimate** (`np.ndarray`): Estimated $m(x_{new})$.\n",
    "    - **confidence_lower** (`float`): Lower bound of the $1-\\alpha$ confidence interval for $y_{new}$.\n",
    "    - **confidence_upper** (`float`): Upper bound of the $1-\\alpha$ confidence interval for $y_{new}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_hat: [-0.02095641  0.98620133  1.01318309 -0.03289706  3.00847295 -1.02679092]\n",
      "R-squared: 0.9259204134491424\n"
     ]
    }
   ],
   "source": [
    "#generate a random dataset\n",
    "np.random.seed(0)\n",
    "n = 1000\n",
    "p = 5\n",
    "X = np.random.randn(n,p)\n",
    "beta = np.array([1,1,0,3,-1])\n",
    "e = np.random.randn(n)\n",
    "y = X @  beta + e\n",
    "\n",
    "#fit an OLS estimator\n",
    "model = OLS(include_intercept=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "#generate new point to predict\n",
    "x_new = np.random.randn(1, p)\n",
    "\n",
    "summ = summary(model, X,y)\n",
    "print(f\"beta_hat: {summ['coefficients']}\")\n",
    "print(f\"R-squared: {summ['r_squared']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient 0:\n",
      "  Estimated: -0.020956408714166208\n",
      "  Null value: 0\n",
      "  t-stat: -0.6704950528825446\n",
      "  p-value: 0.5026980078679999\n",
      "  Reject null: False\n",
      "Coefficient 1:\n",
      "  Estimated: 0.9862013324637019\n",
      "  Null value: 1\n",
      "  t-stat: -0.43719825879662916\n",
      "  p-value: 0.6620625378445069\n",
      "  Reject null: False\n",
      "Coefficient 2:\n",
      "  Estimated: 1.013183086563438\n",
      "  Null value: 1\n",
      "  t-stat: 0.3953575243015688\n",
      "  p-value: 0.6926638823957914\n",
      "  Reject null: False\n",
      "Coefficient 3:\n",
      "  Estimated: -0.0328970595911511\n",
      "  Null value: 0\n",
      "  t-stat: -1.060481138694913\n",
      "  p-value: 0.28918337260347227\n",
      "  Reject null: False\n",
      "Coefficient 4:\n",
      "  Estimated: 3.0084729523193987\n",
      "  Null value: 3\n",
      "  t-stat: 0.2723490663792494\n",
      "  p-value: 0.7854101946438525\n",
      "  Reject null: False\n",
      "Coefficient 5:\n",
      "  Estimated: -1.026790915553867\n",
      "  Null value: -1\n",
      "  t-stat: -0.8484471213816155\n",
      "  p-value: 0.3963932771608605\n",
      "  Reject null: False\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 0:\n",
      "  Estimated: -0.020956408714166208\n",
      "  Confidence interval: [-0.08229001653158752, 0.04037719910325511]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 1:\n",
      "  Estimated: 0.9862013324637019\n",
      "  Confidence interval: [0.9242663588267555, 1.0481363061006483]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 2:\n",
      "  Estimated: 1.013183086563438\n",
      "  Confidence interval: [0.9477489576897662, 1.0786172154371096]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 3:\n",
      "  Estimated: -0.0328970595911511\n",
      "  Confidence interval: [-0.09377099267799052, 0.02797687349568833]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 4:\n",
      "  Estimated: 3.0084729523193987\n",
      "  Confidence interval: [2.94742289143181, 3.0695230132069873]\n",
      "--------------------------------------------------------------------------------\n",
      "Coefficient 5:\n",
      "  Estimated: -1.026790915553867\n",
      "  Confidence interval: [-1.0887549953747322, -0.9648268357330019]\n",
      "--------------------------------------------------------------------------------\n",
      "F-stat: 0.41268836220298705\n",
      "p-value: 0.6619818652712848\n",
      "Reject null: False\n"
     ]
    }
   ],
   "source": [
    "#hypothesis testing on the coefficients\n",
    "\n",
    "tester = LinearModelTester(model)\n",
    "H0 = np.array([0,1,1,0,3,-1])\n",
    "alpha = 0.05\n",
    "\n",
    "# test H0 for each coefficient\n",
    "\n",
    "results = tester.hypothesis_t_test(X, y, H0, alpha)\n",
    "for result in results:\n",
    "    print(f\"Coefficient {result['coefficient']}:\")\n",
    "    print(f\"  Estimated: {result['beta_estimate']}\")\n",
    "    print(f\"  Null value: {result['null_value']}\")\n",
    "    print(f\"  t-stat: {result['t_stat']}\")\n",
    "    print(f\"  p-value: {result['p_value']}\")\n",
    "    print(f\"  Reject null: {result['reject_null']}\")\n",
    "\n",
    "# build confidence intervals for coefficients\n",
    "results = tester.confidence_interval(X, y, alpha)\n",
    "for result in results:\n",
    "    print('--'*40)\n",
    "    print(f\"Coefficient {result['coefficient']}:\")\n",
    "    print(f\"  Estimated: {result['beta_estimate']}\")\n",
    "    print(f\"  Confidence interval: [{result['confidence_lower']}, {result['confidence_upper']}]\")\n",
    "\n",
    "# hypothesis testing on linear combinations of coefficients\n",
    "R = np.array([\n",
    "    [0, 1, -1, 0, 0, 0],  # beta_1 - beta_2 = 0\n",
    "    [0, 0, 2, 0, 0, 2]   # 2*beta_2 + 2*beta_5 = 0\n",
    "])\n",
    "r = [0, 0] \n",
    "\n",
    "# H0 = Rbeta = r\n",
    "\n",
    "results = tester.hypothesis_F_test(X, y, R, r, alpha)\n",
    "print('--'*40)\n",
    "print(f\"F-stat: {results['F_stat']}\")\n",
    "print(f\"p-value: {results['p_value']}\")\n",
    "print(f\"Reject null: {results['reject_null']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Prediction interval for new point m[[ 2.04253623 -0.91946118  0.11467003 -0.1374237   1.36552692]]:\n",
      "  Estimated m(x_new): -0.757505417301134\n",
      "  Confidence interval: [-0.9397209890558961, -0.5752898455463719]\n",
      "--------------------------------------------------------------------------------\n",
      "Prediction interval for response of new point, y_new:\n",
      "  Confidence interval: [-2.6998488506853864, 1.1848380160831187]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "result = tester.prediction_interval_m(X, y, x_new, alpha)\n",
    "print('--'*40)\n",
    "print(f\"Prediction interval for new point m{x_new}:\")\n",
    "print(f\"  Estimated m(x_new): {result['mx_new_estimate']}\")\n",
    "print(f\"  Confidence interval: [{result['confidence_lower']}, {result['confidence_upper']}]\")\n",
    "\n",
    "result = tester.prediction_interval_y(X, y, x_new, alpha)\n",
    "print('--'*40)\n",
    "print(f\"Prediction interval for response of new point, y_new:\")\n",
    "print(f\"  Confidence interval: [{result['confidence_lower']}, {result['confidence_upper']}]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
